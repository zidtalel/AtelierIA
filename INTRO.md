# PrÃ©sentation : IA & LLMs pour coder

[â†©ï¸ Retour au README](README.md)

Objectif : donner une vue courte et pratique des principes des LLMs (Large Language Models) appliquÃ©s au code (ex. GitHub Copilot), ce qu'ils savent faire et leurs limites.

## 1) En une phrase

Les LLMs pour le code prÃ©disent la suite la plus probable Ã  partir du contexte (fichiers ouverts, commentaires, prompt) et suggÃ¨rent du code â€” ils accÃ©lÃ¨rent le dÃ©veloppement, mais la relecture humaine reste indispensable.

Petit rappel utile : un LLM n'Â« exÃ©cute Â» pas le code pendant la gÃ©nÃ©ration. Il choisit des tokens plausibles selon son entraÃ®nement. Donc la qualitÃ© du prompt et du contexte guide fortement la prÃ©cision et la pertinence des rÃ©ponses.

### Note : c'est quoi un *token* ?

Un *token* est une petite unitÃ© de texte que le modÃ¨le manipule. Ce n'est pas toujours un mot entier : cela peut Ãªtre un morceau de mot, un symbole, un espace ou une ponctuation.

Le modÃ¨le "voit" et prÃ©dit une sÃ©quence de tokens, pas des phrases complÃ¨tes d'un coup.

Deux implications pratiques :

1. Longueur / coÃ»t : chaque token compte dans la fenÃªtre de contexte (limite de taille). Plus vous envoyez de texte, plus vous consommez de tokens.
2. PrÃ©cision : des noms explicites (fonctions, variables) et des exemples rÃ©duisent l'ambiguÃ¯tÃ© des prochains tokens Ã  prÃ©dire â†’ meilleure qualitÃ© de sortie.

Astuce : si la rÃ©ponse devient incomplÃ¨te ou coupÃ©e, c'est parfois parce que la limite de tokens de sortie est atteinte.

### Comment un LLM prÃ©dit le prochain token (vue interne simplifiÃ©e)

Points importants : Copilot ne Â« comprend Â» pas le projet comme un humain ; il prÃ©dit la suite la plus probable en se basant sur des patterns vus pendant l'entraÃ®nement.

Le cycle principal de gÃ©nÃ©ration suit une chaÃ®ne dÃ©terministe (avec parfois de l'alÃ©a dans l'Ã©chantillonnage) :

1. Tokenisation : le texte d'entrÃ©e est dÃ©coupÃ© en tokens (morceaux de mots / symboles).
2. Embeddings : chaque token est converti en vecteur dense (reprÃ©sentation numÃ©rique).
3. Position : on ajoute une information de position (sinusoÃ¯dale ou apprise) pour l'ordre.
4. Empilement de couches Transformer : chaque couche applique (a) self-attention (le token "regarde" les prÃ©cÃ©dents) puis (b) un rÃ©seau feed-forward.
5. Logits : la derniÃ¨re couche produit, pour chaque position courante, un score brut (logits) pour tout le vocabulaire.
6. Softmax + TempÃ©rature : transformation des logits en distribution de probabilitÃ©s.
7. Ã‰chantillonnage / DÃ©codage : on choisit le prochain token (greedy, top-k, nucleus, etc.).
8. Boucle : on ajoute ce token au contexte et on recommence jusqu'au stop (fin, longueur max, ou token spÃ©cial).

Diagramme (simplifiÃ©) :

```mermaid
%%{init: {'theme':'dark'}}%%
flowchart TD
  A[1- Prompt_and_Context]
  B[2- Tokenisation]
  C[3- Embeddings_Positions]
  subgraph STACK[4- Transformer_Stack]
    direction TB
    S1[SelfAttn_FFN_C1]
    S2[SelfAttn_FFN_C2]
    S3[...]
    SN[SelfAttn_FFN_CN]
  end
  D[5- Logits_vocab]
  E[6- Softmax_Temperature]
  F[7- Decoding]
  G[7- Token_chosen]
  H[8- Add_to_context]
  I[8- Stop]
  J[8- Final_output]

  A --> B --> C --> STACK --> D --> E --> F --> G --> H --> I
  I -- No --> B
  I -- Yes --> J

  classDef block fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#e2e8f0
  classDef focus fill:#553c9a,stroke:#805ad5,stroke-width:2px,color:#e2e8f0
  classDef action fill:#276749,stroke:#38a169,stroke-width:2px,color:#e2e8f0
  class A,B,C,S1,S2,S3,SN,D,E,F,G,H,I,J block
  class F focus
  class G,H action
```

Lecture rapide : le modÃ¨le ne produit pas un bloc entier d'un coup, mais un token Ã  la fois, rÃ©Ã©valuant la distribution complÃ¨te Ã  chaque itÃ©ration.

## 2) Flux simplifiÃ©

```mermaid
%%{init: {'theme':'dark'}}%%
flowchart TD
subgraph subGraph0 ["Flux"]
  direction LR
    D["DonnÃ©es (code + doc)"] --> P["PrÃ©-entrainement"]
    P --> FT["Fine-tuning"]
    FT --> SUG["Suggestions (Copilot)"]
  n1["Exemples open-source"] --- n2["Apprend les motifs & la syntaxe"]
    n2 --- n4["Ajuste aux tÃ¢ches"]
    n4 --- n3["Auto-complÃ©tion / Tests"]
end

    n1@{ shape: text}
    n2@{ shape: text}
    n4@{ shape: text}
    n3@{ shape: text}
     D:::Sky
     P:::Peach
     FT:::Aqua
     SUG:::Rose
    classDef Sky stroke-width:1px, stroke-dasharray:none, stroke:#374D7C, fill:#E2EBFF, color:#374D7C
    classDef Peach stroke-width:1px, stroke-dasharray:none, stroke:#FBB35A, fill:#FFEFDB, color:#8F632D
    classDef Aqua stroke-width:1px, stroke-dasharray:none, stroke:#46EDC8, fill:#DEFFF8, color:#378E7A
    classDef Rose stroke-width:1px, stroke-dasharray:none, stroke:#FF5978, fill:#FFDFE5, color:#8E2236
    style subGraph0 fill:transparent
    linkStyle 3 stroke:none,fill:none
    linkStyle 4 stroke:none,fill:none
    linkStyle 5 stroke:none
```

Points clefs : prÃ©-entrainement sur un large corpus, fine-tuning pour tÃ¢ches spÃ©cifiques, et fenÃªtre de contexte limitÃ©e (longueur maximale). Ces limites expliquent pourquoi il faut fournir un contexte ciblÃ©.


## 3) Usage concret de l'IA pour coder

- Auto-complÃ©tion (lignes, blocs, fonctions)
- GÃ©nÃ©ration de tests unitaires et exemples d'utilisation
- Aide au refactoring et traduction entre langages
- **GÃ©nÃ©ration de logiciels basÃ©s sur des exigences textuelles**

## 4) Pourquoi le contexte & la variabilitÃ© importent

```mermaid
%%{init: {'theme':'dark'}}%%
flowchart TD
subgraph subGraph0 ["EntrÃ©es"]
  direction LR
  PC["Prompt / Contexte"] --> PARM["ParamÃ¨tres (tempÃ©rature, seed)"]
end
subgraph subGraph1 ["Sorties"]
  direction TB
  A_OUT["Sortie A (variante)"]
  B_OUT["Sortie B (variante)"]
  C_OUT["Sortie C (variante)"]
end
  %% Replace labeled link with an explicit transparent label node so label background is none
  EXPL["MÃªme prompt + paramÃ¨tres diffÃ©rents â†’ sorties diffÃ©rentes"]
  EXPL@{ shape: text}
  subGraph0 --- EXPL ---> subGraph1
  classDef block1 stroke-width:1px, stroke-dasharray:none, stroke:#374D7C, fill:#E2EBFF, color:#374D7C
  classDef block2 stroke-width:1px, stroke-dasharray:none, stroke:#46EDC8, fill:#DEFFF8, color:#378E7A
  classDef explClass fill:none, stroke:none, color:#FFD700
  class PC,PARM block1
  class A_OUT,B_OUT,C_OUT block2
  class EXPL explClass
  style subGraph0 fill:transparent,stroke:#2962FF
  style subGraph1 fill:transparent,stroke:#00C853
```

- Contexte : noms de fonctions, commentaires, fichiers ouverts, exemples d'entrÃ©e/sortie.
- VariabilitÃ© : paramÃ¨tres comme la "tempÃ©rature" et la seed modifient les sorties (plus ou moins alÃ©atoires).

Conseil pratique : fournir un prompt structurÃ©, des exemples, et indiquer le format exact attendu.

## 5) Guide bref â€” Ã‰crire un bon prompt (pour dÃ©butant)

Un bon prompt contient gÃ©nÃ©ralement ces Ã©lÃ©ments (ordre recommandÃ©) :

1) Objectif clair : que doit produire l'IA ? (ex. "Ã‰crire une fonction Java qui...")
2) Contrainte(s) : langage, version, style, performance, bibliothÃ¨ques Ã  utiliser ou Ã  Ã©viter.
3) EntrÃ©e(s) et sortie(s) attendues : schÃ©ma, types, exemples concrets.
4) CritÃ¨res d'Ã©valuation ou tests rapides : cas limite, complexitÃ© attendue, tests unitaires simples.
5) Exemple(s) : un petit exemple d'entrÃ©e â†’ sortie pour guider le modÃ¨le.

Template court (Ã  copier) :

"Tu es un assistant expert en [langage]. Objectif : [but prÃ©cis]. Contraintes : [langage/version], ne pas utiliser [lib], respecter [style]. EntrÃ©e : [description]. Sortie attendue : [format]. Exemple : [entrÃ©e] â†’ [sortie]."

Exemple concret :

"Tu es un assistant Java. Objectif : Ã©crire une mÃ©thode statique 'truncate(String text, int n)' qui coupe une chaÃ®ne Ã  n caractÃ¨res en ajoutant '...' si nÃ©cessaire. Contraintes : Java 11+, pas de dÃ©pendance externe. EntrÃ©e : String, int. Sortie : String. Exemple : 'Bonjour', 3 â†’ 'Bon...' ."

Pourquoi c'est important :

- PrÃ©cision : plus vous donnez d'informations pertinentes (contraintes, exemples), plus la sortie sera proche de ce que vous attendez.
- RÃ©duction des erreurs : indiquer des tests ou des cas limites aide le modÃ¨le Ã  Ã©viter les hallucinations.
- ItÃ©ration rapide : commencez par un prompt prÃ©cis, vÃ©rifiez la sortie, puis demandez des corrections ciblÃ©es (ex. "corrige pour les cas oÃ¹...").

## 6) Limites rapides (Ã  garder en tÃªte)

- Hallucinations : le modÃ¨le peut inventer des fonctions, signatures ou APIs â€” toujours vÃ©rifier et exÃ©cuter le code.
- Licence / provenance : attention si vous intÃ©grez du code sans vÃ©rifier la provenance.
- SÃ©curitÃ© : ne pas exposer de secrets ni de donnÃ©es sensibles dans les prompts.

## 7) Vibe Coding avec l'IA (exploration rapide)

Le "Vibe Coding" (codage par flux ou exploration assistÃ©e) dÃ©signe des micro-itÃ©rations trÃ¨s rapides oÃ¹ l'on gÃ©nÃ¨re, teste et ajuste du code avec l'IA sans concevoir immÃ©diatement une solution formelle. Objectif : maximiser la vitesse d'apprentissage et de prototypage tout en limitant les risques (qualitÃ©, sÃ©curitÃ©, licence).

### Quand l'utiliser

- Prototype / preuve de concept
- Exploration API / librairie inconnue
- Recherche de patterns de refactoring
- GÃ©nÃ©ration d'idÃ©es de tests ou de cas limites

### Boucle typique (30â€“120s)

```mermaid
%%{init: {'theme':'dark'}}%%
flowchart LR
  IDEA[IdÃ©e] --> PROMPT[Prompt rapide]
  PROMPT --> AI[Suggestion IA]
  AI --> TEST[Test rapide]
  TEST --> DECIDE{OK?}
  DECIDE -->|Oui| COMMIT[Commit local]
  DECIDE -->|Non| REFINE[Affiner prompt]
  REFINE --> PROMPT

  classDef base fill:#2d3748,stroke:#4a5568,color:#e2e8f0
  classDef action fill:#276749,stroke:#38a169,color:#e2e8f0
  class IDEA,PROMPT,AI,TEST,REFINE base
  class COMMIT action
```

ClÃ©s :

- Prompt simple et ciblÃ©
- Tester vite (unitaires / lint)
- Accepter si sÃ»r, sinon ajuster le prompt
- Commit local puis structurer si besoin

> Cycle court â€” privilÃ©giez petites itÃ©rations et feedback automatique.

## 8) DÃ©veloppement pilotÃ© par spÃ©cifications (Spec-Driven Development)

Le dÃ©veloppement pilotÃ© par spÃ©cifications consiste Ã  rÃ©diger une spec claire (objectifs, interface, exemples, tests) avant de demander Ã  un LLM de gÃ©nÃ©rer le code. C'est une approche utile pour les fonctionnalitÃ©s critiques ou partagÃ©es oÃ¹ la fiabilitÃ© et la traÃ§abilitÃ© sont importantes.

Principaux bÃ©nÃ©fices :

- Clarifie les exigences et rÃ©duit les malentendus.
- Produit du code plus testable et maintenable.
- Facilite la revue collaborative et la traÃ§abilitÃ© entre spec et code.

Checklist minimale pour une spec efficace :

1. Objectif bref et contexte
2. Signature / interface (types)
3. 2â€“3 exemples entrÃ©eâ†’sortie
4. Tests essentiels / cas limites

Mini-template Ã  copier :

```markdown
# Spec: [Nom]
## Contexte
Objectif : []
## Interface
- Langage: Java 11+
- Signature: `public static <Type> f(<params>)`
  (ex: `public static String truncate(String text, int n)`)
## Exemples
- EntrÃ©e: [...] -> Sortie: [...]
## Tests
- Cas nominal
- Cas limite
```

Workflow rÃ©sumÃ© : RÃ©diger spec â†’ GÃ©nÃ©rer via LLM â†’ ExÃ©cuter tests â†’ Revue humaine et commit.

### Workflow Spec-Driven avec LLM

```mermaid
%%{init: {'theme':'dark'}}%%
flowchart LR
  SPEC["Spec claire"] --> GEN["GÃ©nÃ©ration (LLM)"]
  GEN --> TESTS["ExÃ©cution des tests"]
  TESTS --> DECIDE{"Tests OK ?"}
  DECIDE -->|Non| FIX["Corriger / Ajuster prompt"] --> GEN
  DECIDE -->|Oui| HUMAN["Revue humaine"] --> COMMIT["Commit + Spec"]
  COMMIT --> MAINT["Maintenance"]
  classDef main fill:#2b6cb0,stroke:#3182ce,color:#e2e8f0
  class SPEC,GEN,TESTS,HUMAN,COMMIT main
```

### Quand privilÃ©gier Spec-Driven vs Vibe Coding

| **CritÃ¨re** | **Spec-Driven** | **Vibe Coding** |
|-------------|----------------|-----------------|
| **Type de projet** | Production, bibliothÃ¨que partagÃ©e | Prototype, POC, exploration |
| **CriticitÃ©** | Code critique (sÃ©curitÃ©, finance) | ExpÃ©rimentation, apprentissage |
| **Ã‰quipe** | Collaboration multi-dev | DÃ©veloppement solo |
| **Documentation** | Requise et maintenue | Optionnelle |
| **Tests** | Complets et automatisÃ©s | Tests exploratoires |
| **Ã‰volutivitÃ©** | Long terme | Court terme |

[ğŸ’« Toolkit to help you get started with Spec-Driven Development](https://github.com/github/spec-kit)

### (Annexe) FenÃªtre de contexte : entrÃ©e / sortie (estimation)

Ce tableau indique, pour chaque modÃ¨le, une estimation de la fenÃªtre totale (tokens), puis une estimation typique de la quantitÃ© maximale utilisable pour l'entrÃ©e (tokens d'entrÃ©e) et pour la sortie gÃ©nÃ©rÃ©e. Rappel : entrÃ©e + sortie â‰¤ fenÃªtre totale.

| ModÃ¨le (exemples)         | FenÃªtre totale (tokens) | Tokens d'entrÃ©e max (approx.) | Tokens sortie max (approx.) | Commentaire |
|---------------------------|------------------------:|------------------------------:|----------------------------:|------------|
| GPT-4.1 (Copilot)         | â‰ˆ128k                  | â‰ˆ96kâ€“120k                     | â‰ˆ8kâ€“32k                     | IntÃ©grÃ© Ã  Copilot, bon pour multi-fichiers et suggestions contextuelles |
| GPT-5 Mini                | â‰ˆ64k                   | â‰ˆ56kâ€“60k                      | â‰ˆ4kâ€“8k                      | Variante allÃ©gÃ©e de GPT-5, Ã©conomique pour usages frÃ©quents |
| Cloud Sonnet 4            | â‰ˆ200k                  | â‰ˆ176kâ€“192k                    | â‰ˆ8kâ€“24k                     | ConÃ§u pour documents volumineux et code long |
| Cloud Sonnet 4.5          | â‰ˆ500k                  | â‰ˆ452kâ€“488k                    | â‰ˆ12kâ€“48k                    | FenÃªtre Ã©tendue pour trÃ¨s grands contextes |
| GPT-5                     | â‰ˆ1M+                   | â‰ˆ800kâ€“950k                    | â‰ˆ50kâ€“200k+                  | Configurations endpoint-dependent ; forte capacitÃ© pour larges projets |
| Grok Code Fast 1         | â‰ˆ64k                   | â‰ˆ56kâ€“60k                      | â‰ˆ4kâ€“8k                      | OptimisÃ© pour le code, faible latence pour suggestions rapides |
| Mistral (dernier)         | â‰ˆ64k (varie)           | â‰ˆ52kâ€“60k                      | â‰ˆ4kâ€“12k                     | Variantes optimisÃ©es pour le code et le coÃ»t |
| Llama 3 (dernier)         | â‰ˆ128kâ€“512k             | â‰ˆ96kâ€“480k                     | â‰ˆ8kâ€“32k                     | Versions et builds variables (open-source/entreprise) |
| Gemini 2.5                | â‰ˆ1M                    | â‰ˆ800kâ€“950k                    | â‰ˆ50kâ€“200k+                  | OrientÃ© trÃ¨s large contexte, utile pour projets multi-fichiers |

Note : estimations (oct. 2025) â€” les valeurs rÃ©elles dÃ©pendent de l'endpoint, des limites imposÃ©es par le fournisseur et des configurations de modÃ¨le. "Tokens d'entrÃ©e max" = fenÃªtre totale âˆ’ tokens rÃ©servÃ©s pour la sortie ; les plages donnÃ©es indiquent des allocations typiques selon usage (conservateur â†’ agressif).

Astuce pratique : si vous avez besoin d'analyser de longs dÃ©pÃ´ts, privilÃ©giez les modÃ¨les Ã  grande fenÃªtre ou prÃ©traitez / rÃ©sumez le code pour n'envoyer que les parties essentielles (signatures, tests, exemples).

---
