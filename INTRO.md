# PrÃ©sentation : L'IA pour coder avec GitHub Copilot

[â†©ï¸ Retour au README](README.md)

**â±ï¸ DurÃ©e estimÃ©e : 45â€“60 minutes**

Objectif : donner une vue courte et pratique des principes des LLMs (Large Language Models) appliquÃ©s Ã  GitHub Copilot, ce qu'ils savent faire et leurs limites.

---

## ğŸ¯ C'est quoi l'IA ? (en 2 minutes)

**L'Intelligence Artificielle (IA)** dÃ©signe des programmes capables d'imiter certaines capacitÃ©s humaines : comprendre du texte, reconnaÃ®tre des images, prendre des dÃ©cisions.

### Analogie simple

Imaginez un **assistant trÃ¨s cultivÃ©** qui a lu des millions de livres, de codes et de documents. Il peut vous aider Ã  Ã©crire, mais :
- Il ne Â« pense Â» pas vraiment â€” il prÃ©dit ce qui semble logique.
- Il peut se tromper ou inventer des choses.
- Il a besoin de vos instructions claires pour bien vous aider.

### Les types d'IA que vous rencontrerez

| Type | Description | Exemple |
|------|-------------|----------|
| **LLM** (Large Language Model) | ModÃ¨le entraÃ®nÃ© sur du texte pour gÃ©nÃ©rer du langage | GPT-4, Claude, Copilot |
| **IA GÃ©nÃ©rative** | CrÃ©e du contenu (texte, images, code) | DALL-E, Midjourney, Copilot |
| **IA de code** | SpÃ©cialisÃ©e pour aider Ã  programmer | GitHub Copilot |

> ğŸ’¡ **Ã€ retenir** : GitHub Copilot est un LLM spÃ©cialisÃ© pour le code. Il prÃ©dit la suite la plus probable de ce que vous Ã©crivez.

---

## 1) En profond : Comment fonctionne un LLM pour le code ?

Les LLMs pour le code prÃ©disent la suite la plus probable Ã  partir du contexte (fichiers ouverts, commentaires, prompt) et suggÃ¨rent du code â€” ils accÃ©lÃ¨rent le dÃ©veloppement, mais la relecture humaine reste indispensable.

> âš ï¸ **Point clÃ©** : Un LLM n'Â« exÃ©cute Â» pas le code. Il prÃ©dit les tokens les plus plausibles selon son entraÃ®nement.

### C'est quoi un *token* ?

Un *token* est une petite unitÃ© de texte (â‰ˆ un morceau de mot, symbole ou ponctuation). Le modÃ¨le gÃ©nÃ¨re une sÃ©quence de tokens, pas des phrases complÃ¨tes.

**Implications pratiques :**

1. Longueur / coÃ»t : chaque token compte dans la fenÃªtre de contexte (limite de taille). Plus vous envoyez de texte, plus vous consommez de tokens.
2. PrÃ©cision : des noms explicites (fonctions, variables) et des exemples rÃ©duisent l'ambiguÃ¯tÃ© des prochains tokens Ã  prÃ©dire â†’ meilleure qualitÃ© de sortie.

Astuce : si la rÃ©ponse devient incomplÃ¨te ou coupÃ©e, c'est parfois parce que la limite de tokens de sortie est atteinte.

> ğŸ’¡ **Ã€ retenir** : Un token â‰ˆ un morceau de mot. Plus votre prompt est long, plus vous consommez de tokens. Soyez concis et prÃ©cis !

### Comment un LLM prÃ©dit le prochain token (vue interne simplifiÃ©e)

<details open>
<summary>ğŸ“š <strong>Section technique â€” Cliquez pour dÃ©velopper</strong> (optionnel pour dÃ©butants)</summary>

#### Analogie : le jeu de devinette

Imaginez que vous jouez Ã  deviner le prochain mot d'une phrase. Si quelqu'un dit Â« Il fait beau, je vais me promener dans le... Â», vous pensez probablement Ã  Â« parc Â», Â« jardin Â» ou Â« quartier Â». Le LLM fait pareil, mais avec des calculs mathÃ©matiques sur des millions d'exemples !

#### Le cycle de gÃ©nÃ©ration (simplifiÃ©)

**Voici comment le modÃ¨le gÃ©nÃ¨re du code, Ã©tape par Ã©tape :**

1. **Tokenisation** : DÃ©coupe votre texte en petits morceaux (tokens)
   - *Exemple* : `"fonction calculer"` â†’ `["fonction", " ", "calcul", "er"]`

2. **Embeddings** : Convertit chaque token en nombres que l'ordinateur peut traiter
   - *Analogie* : Comme traduire des mots en coordonnÃ©es GPS pour les manipuler

3. **Positions** : Ajoute l'information de l'ordre des mots
   - *Pourquoi* : Le modÃ¨le doit savoir que "Java aime Pierre" â‰  "Pierre aime Java"

4. **Couches Transformer** : Le cÅ“ur du modÃ¨le qui analyse le contexte
   - **Attention** : Le token "regarde en arriÃ¨re" pour comprendre le contexte
     - *Example* : Pour complÃ©ter `data.get___`, il regarde que `data` est une Map â†’ propose `(key)`
   - **RÃ©seau** : Calcule et transforme l'information pour extraire des patterns
     - *Analogie* : Comme un calculateur qui combine les indices trouvÃ©s

5. **Scores** : Calcule un score pour chaque mot possible du vocabulaire entier
   - *Example* : `public` (score: 0.8), `private` (score: 0.6), `banana` (score: 0.001)

6. **ProbabilitÃ©s** : Transforme les scores en pourcentages (0-100%)
   - *Example* : `public` (45%), `private` (35%), autres (20%)

7. **DÃ©codage** : Choisit le prochain token selon une stratÃ©gie
   - **Greedy** : Toujours le plus probable
   - **Top-k** : Parmi les k meilleurs
   - **Nucleus** : Parmi ceux qui totalisent x% de probabilitÃ©

8. **Boucle** : Ajoute ce token au contexte et recommence jusqu'Ã  la fin
   - Le nouveau token devient partie du contexte pour prÃ©dire le suivant

**Diagramme du cycle :**

```mermaid
%%{init: {'theme':'dark'}}%%
flowchart TD
  A["1. Prompt et Contexte"]
  B["2. Tokenisation"]
  C["3. Embeddings + Positions"]
  subgraph STACK["4. Couches Transformer"]
    direction TB
    S1["Attention + RÃ©seau"]
    S2["..."]
    SN["Attention + RÃ©seau"]
  end
  D["5. Scores"]
  E["6. ProbabilitÃ©s"]
  F["7. DÃ©codage"]
  G["8. Token choisi"]
  H{"Fini ?"}
  J["Sortie finale"]

  A --> B --> C --> STACK --> D --> E --> F --> G --> H
  H -- Non, ajouter au contexte --> B
  H -- Oui --> J

  classDef start fill:#276749,stroke:#38a169,stroke-width:3px,color:#e2e8f0
  classDef process fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#e2e8f0
  classDef transformer fill:#44337a,stroke:#6b46c1,stroke-width:2px,color:#e2e8f0
  classDef focus fill:#553c9a,stroke:#805ad5,stroke-width:2px,color:#e2e8f0
  classDef final fill:#742a2a,stroke:#e53e3e,stroke-width:3px,color:#e2e8f0
  class A,G start
  class B,C,D,E,H process
  class STACK,S1,S2,SN transformer
  class F focus
  class J final
```

</details>

> ğŸ’¡ **Ã€ retenir** : Copilot gÃ©nÃ¨re le code token par token (mot par mot). Il prÃ©dit ce qui semble le plus probable, il ne "comprend" pas vraiment.

## 2) Flux d'apprentissage simplifiÃ©

**Diagramme du flux d'apprentissage :**

```mermaid
%%{init: {'theme':'dark'}}%%
flowchart TD
subgraph subGraph0 ["Flux d'apprentissage"]
  direction LR
    D["ğŸ“š DonnÃ©es<br/>(code + doc)"] --> P["ğŸ§  PrÃ©-entrainement"]
    P --> FT["âš™ï¸ Fine-tuning"]
    FT --> SUG["ğŸ’¡Suggestions<br/> dans Copilot"]
    
  n1["Millions de lignes<br/>de code public"] --- n2["Apprend les motifs<br/> & la syntaxe"]
    n2 --- n4["Ajuste aux <br/>tÃ¢ches spÃ©cifiques (code, textes, etc, images.)"]
    n4 --- n3["Auto-complÃ©tion<br/> GÃ©nÃ©ration code<br/> Tests"]
end

    n1@{ shape: text}
    n2@{ shape: text}
    n4@{ shape: text}
    n3@{ shape: text}
     D:::dataClass
     P:::learnClass
     FT:::learnClass
     SUG:::outputClass

    classDef dataClass fill:#2b6cb0,stroke:#3182ce,stroke-width:2px,color:#e2e8f0
    classDef learnClass fill:#553c9a,stroke:#805ad5,stroke-width:2px,color:#e2e8f0
    classDef outputClass fill:#276749,stroke:#38a169,stroke-width:2px,color:#e2e8f0

    style subGraph0 fill:transparent
    linkStyle 3 stroke:none,fill:none
    linkStyle 4 stroke:none,fill:none
    linkStyle 5 stroke:none
```
**Comment Copilot apprend Ã  gÃ©nÃ©rer du code :**

1. **DonnÃ©es (code + documentation)** : Collecte massive de code source
   - *Sources* : Projets open-source (GitHub), documentation technique, exemples de code
   - *Volumes* : Milliards de lignes de code dans diffÃ©rents langages
   - *DiversitÃ©* : Du code dÃ©butant au code expert, diffÃ©rents styles et domaines

2. **PrÃ©-entraÃ®nement** : Apprentissage des motifs et de la syntaxe
   - *Objectif* : Apprendre les patterns gÃ©nÃ©raux de programmation
   - *Processus* : Le modÃ¨le lit des millions d'exemples et apprend Ã  prÃ©dire le code suivant
   - *RÃ©sultat* : Comprend la syntaxe, les conventions, les patterns communs
   - *Analogie* : Comme apprendre Ã  lire en lisant des milliers de livres

3. **Fine-tuning** : Ajustement aux tÃ¢ches spÃ©cifiques
   - *Objectif* : SpÃ©cialiser le modÃ¨le pour des tÃ¢ches prÃ©cises (auto-complÃ©tion, gÃ©nÃ©ration de tests, etc.)
   - *Processus* : EntraÃ®nement supplÃ©mentaire sur des exemples ciblÃ©s
   - *Optimisation* : AmÃ©liore la qualitÃ© pour les cas d'usage courants
   - *Analogie* : Comme un mÃ©decin gÃ©nÃ©raliste qui se spÃ©cialise en chirurgie

4. **Suggestions (Copilot)** : Utilisation en production
   - *Sur VS Code* : Auto-complÃ©tion en temps rÃ©el pendant que vous codez
   - *Dans le Chat* : RÃ©ponses Ã  vos questions et gÃ©nÃ©ration de code
   - *GÃ©nÃ©ration de tests* : CrÃ©ation automatique de tests unitaires
   - *Limitation* : FenÃªtre de contexte limitÃ©e (ne voit qu'une partie de votre projet)




> ğŸ’¡ **Ã€ retenir** : Copilot est entraÃ®nÃ© sur des milliards de lignes de code, mais a une **fenÃªtre de contexte limitÃ©e**. C'est pourquoi donner un contexte clair et ciblÃ© amÃ©liore grandement les rÃ©sultats !


## 3) Usage concret de l'IA pour coder

Voici ce que GitHub Copilot peut faire pour vous :

| ğŸ¯ FonctionnalitÃ© | ğŸ“ Description | âŒ¨ï¸ Comment l'utiliser |
|------------------|----------------|----------------------|
| **Auto-complÃ©tion** | SuggÃ¨re la suite de votre code | Tapez et attendez les suggestions grises |
| **GÃ©nÃ©ration de fonctions** | CrÃ©e des fonctions complÃ¨tes | Ã‰crivez un commentaire dÃ©crivant la fonction |
| **Tests unitaires** | GÃ©nÃ¨re des tests pour votre code | Demandez dans le chat : Â« gÃ©nÃ¨re des tests pour... Â» |
| **Refactoring** | AmÃ©liore et simplifie le code | SÃ©lectionnez du code + demandez une amÃ©lioration |
| **Documentation** | Ajoute des commentaires explicatifs | Demandez : Â« documente cette fonction Â» |

> ğŸ’¡ **Ã€ retenir** : Copilot est un assistant, pas un remplacement. Relisez et testez toujours le code gÃ©nÃ©rÃ© !

## 4) Pourquoi le contexte & la variabilitÃ© importent

#### ğŸ“ Notion clÃ© : le "contexte"

Le **contexte** = tout ce que l'IA peut voir pour faire sa suggestion :

- Le fichier que vous Ã©ditez
- Les autres fichiers ouverts dans VS Code
- Vos commentaires et noms de variables
- Le prompt que vous Ã©crivez


>L'espace de contexte inclu aussi **l'espace de travail interne de l'IA**



```mermaid
%%{init: {'theme':'dark'}}%%
flowchart TD
subgraph subGraph0 ["EntrÃ©es"]
  direction LR
  PC["Prompt / Contexte"] --> PARM["ParamÃ¨tres (tempÃ©rature, seed)"]
end
subgraph subGraph1 ["Sorties"]
  direction TB
  A_OUT["Sortie A (variante)"]
  B_OUT["Sortie B (variante)"]
  C_OUT["Sortie C (variante)"]
end
  %% Replace labeled link with an explicit transparent label node so label background is none
  EXPL["MÃªme prompt + paramÃ¨tres diffÃ©rents â†’ sorties diffÃ©rentes"]
  EXPL@{ shape: text}
  subGraph0 --- EXPL ---> subGraph1
  classDef block1 stroke-width:1px, stroke-dasharray:none, stroke:#374D7C, fill:#E2EBFF, color:#374D7C
  classDef block2 stroke-width:1px, stroke-dasharray:none, stroke:#46EDC8, fill:#DEFFF8, color:#378E7A
  classDef explClass fill:none, stroke:none, color:#FFD700
  class PC,PARM block1
  class A_OUT,B_OUT,C_OUT block2
  class EXPL explClass
  style subGraph0 fill:transparent,stroke:#2962FF
  style subGraph1 fill:transparent,stroke:#00C853
```

**Ce qui influence les rÃ©sultats :**
- **Contexte** : noms explicites, commentaires clairs, fichiers ouverts, exemples concrets
- **ParamÃ¨tres** : tempÃ©rature (crÃ©ativitÃ©) et seed (variabilitÃ© alÃ©atoire)

> ğŸ’¡ **Ã€ retenir** : Plus votre contexte est prÃ©cis et structurÃ©, meilleurs sont les rÃ©sultats.

## 5) Guide bref â€” Ã‰crire un bon prompt (pour dÃ©butant)

### ğŸ”‘ Les 6 ingrÃ©dients d'un bon prompt

Un bon prompt contient gÃ©nÃ©ralement ces Ã©lÃ©ments (ordre recommandÃ©) :

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ğŸ¯ OBJECTIF    â†’ Que doit produire l'IA ?           â”‚
â”‚  2. âš™ï¸ CONTRAINTES â†’ Langage, version, style            â”‚
â”‚  3. ğŸ“¥ ENTRÃ‰ES     â†’ Quelles donnÃ©es en entrÃ©e ?        â”‚
â”‚  4. ğŸ“¤ SORTIES     â†’ Quel format attendu ?              â”‚
â”‚  5. ğŸ’¡ EXEMPLE     â†’ Un cas concret entrÃ©e â†’ sortie     â”‚
â”‚  6. âš ï¸ CAS LIMITE  â†’ Cas limite ou supplÃ©mentaire       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DÃ©tails de chaque ingrÃ©dient

1) Objectif clair : que doit produire l'IA ? (ex. "Ã‰crire une fonction Java qui...")
2) Contrainte(s) : langage, version, style, performance, bibliothÃ¨ques Ã  utiliser ou Ã  Ã©viter.
3) EntrÃ©e(s) et sortie(s) attendues : schÃ©ma, types, exemples concrets.
4) CritÃ¨res d'Ã©valuation ou tests rapides : cas limite, complexitÃ© attendue, tests unitaires simples.
5) Exemple(s) : un petit exemple d'entrÃ©e â†’ sortie pour guider le modÃ¨le.
6) Cas limite(s) : mentionner des scÃ©narios particuliers Ã  gÃ©rer. (optionnel)

Template court (Ã  copier) :

```text
"Tu es un assistant expert en [langage]. Objectif : [but prÃ©cis]. Contraintes : [langage/version], ne pas utiliser [lib], respecter [style]. EntrÃ©e : [description]. Sortie attendue : [format]. Exemple : [entrÃ©e] â†’ [sortie]."
```

**Exemple concret :**

```text
Tu es un expert codeur Java. Objectif : Ã©crire une mÃ©thode statique 'truncate(String text, int n)' 
qui coupe une chaÃ®ne Ã  n caractÃ¨res en ajoutant '...' si nÃ©cessaire. 
Contraintes : Java 11+, pas de dÃ©pendance externe. 
EntrÃ©e : String, int. Sortie : String. 
Exemple : 'Bonjour', 3 â†’ 'Bon...'
Test : GÃ©rer le cas oÃ¹ n > longueur du texte.
```

> ğŸ’¡ **Ã€ retenir** : Plus vous Ãªtes prÃ©cis (contraintes, exemples, cas limites), moins le modÃ¨le invente et meilleurs sont les rÃ©sultats.

## 6) âš ï¸ Limites rapides (Ã  garder en tÃªte)

> **Important** : Ces limites sont cruciales Ã  comprendre avant d'utiliser Copilot !

| âš ï¸ Limite | ğŸ“ Description | âœ… Bonne pratique |
|-----------|----------------|-------------------|
| **ğŸ­ Hallucinations** | Le modÃ¨le peut inventer des fonctions, signatures ou APIs qui n'existent pas | Toujours vÃ©rifier et exÃ©cuter le code |
| **ğŸ“œ Licence / provenance** | Le code gÃ©nÃ©rÃ© peut ressembler Ã  du code sous licence | VÃ©rifier la provenance avant intÃ©gration |
| **ğŸ”’ SÃ©curitÃ©** | Ne jamais exposer de donnÃ©es sensibles | Pas de mots de passe, clÃ©s API ou secrets dans les prompts |
| **ğŸ”„ Non-dÃ©terministe** | Deux fois le mÃªme prompt peut donner des rÃ©sultats diffÃ©rents | ItÃ©rer et affiner vos prompts |

> ğŸ’¡ **Ã€ retenir** : Copilot est un outil puissant mais faillible. Vous restez responsable du code que vous livrez !

## 7) Vibe Coding avec l'IA (exploration rapide)

Le "Vibe Coding" (codage par flux ou exploration assistÃ©e) dÃ©signe des micro-itÃ©rations trÃ¨s rapides oÃ¹ l'on gÃ©nÃ¨re, teste et ajuste du code avec l'IA sans concevoir immÃ©diatement une solution formelle. Objectif : maximiser la vitesse d'apprentissage et de prototypage tout en limitant les risques (qualitÃ©, sÃ©curitÃ©, licence).

### Quand l'utiliser

- Prototype / preuve de concept
- Exploration API / librairie inconnue
- Recherche de patterns de refactoring
- GÃ©nÃ©ration d'idÃ©es de tests ou de cas limites

### Boucle typique (30â€“120s)

```mermaid
%%{init: {'theme':'dark'}}%%
flowchart LR
  IDEA[ğŸ’¡ IdÃ©e] -->A
    A["ğŸ’­ DÃ©crire<br/>ce que vous voulez"] --> B["ğŸ¤– Copilot<br/>suggÃ¨re du code"]
    B --> C["ğŸ‘€ Vous relisez<br/>et testez"]
    C --> D{"âœ… OK ?"}
    D -->|Oui| E["âœ”ï¸ Accepter"]
    D -->|Non| F["âœï¸ Affiner<br/>le prompt"]
    F --> A
    
    classDef userClass fill:#2b6cb0,stroke:#3182ce,stroke-width:2px,color:#e2e8f0
    classDef aiClass fill:#553c9a,stroke:#805ad5,stroke-width:2px,color:#e2e8f0
    classDef decisionClass fill:#744210,stroke:#d69e2e,stroke-width:3px,color:#e2e8f0
    classDef successClass fill:#276749,stroke:#38a169,stroke-width:2px,color:#e2e8f0
    
    class A,C,F,IDEA userClass
    class B aiClass
    class D decisionClass
    class E successClass
```

ClÃ©s :

- Prompt simple et ciblÃ©
- Tester vite (unitaires / lint)
- Accepter si sÃ»r, sinon ajuster le prompt
- Commit local puis structurer si besoin

> Cycle court â€” privilÃ©giez petites itÃ©rations et feedback automatique.

## 8) DÃ©veloppement pilotÃ© par spÃ©cifications (Spec-Driven Development)

Le dÃ©veloppement pilotÃ© par spÃ©cifications consiste Ã  rÃ©diger une spec claire (objectifs, interface, exemples, tests) avant de demander Ã  un LLM de gÃ©nÃ©rer le code. C'est une approche utile pour les fonctionnalitÃ©s critiques ou partagÃ©es oÃ¹ la fiabilitÃ© et la traÃ§abilitÃ© sont importantes.

Principaux bÃ©nÃ©fices :

- Clarifie les exigences et rÃ©duit les malentendus.
- Produit du code plus testable et maintenable.
- Facilite la revue collaborative et la traÃ§abilitÃ© entre spec et code.

Checklist minimale pour une spec efficace :

1. Objectif bref et contexte
2. Signature / interface (types)
3. 2â€“3 exemples entrÃ©eâ†’sortie
4. Tests essentiels / cas limites

Mini-template Ã  copier :

```markdown
# Spec: [Nom]
## Contexte
Objectif : []
## Interface
- Langage: Java 11+
- Signature: `public static <Type> f(<params>)`
  (ex: `public static String truncate(String text, int n)`)
## Exemples
- EntrÃ©e: [...] -> Sortie: [...]
## Tests
- Cas nominal
- Cas limite
```

Workflow rÃ©sumÃ© : RÃ©diger spec â†’ GÃ©nÃ©rer via LLM â†’ ExÃ©cuter tests â†’ Revue humaine et commit.

### Workflow Spec-Driven avec LLM

```mermaid
%%{init: {'theme':'dark'}}%%
flowchart LR
  SPEC["SpÃ©cification claire"] --> GEN["GÃ©nÃ©ration (LLM)"]
  GEN --> TESTS["ExÃ©cution des tests"]
  TESTS --> DECIDE{"Tests OK ?"}
  DECIDE -->|Non| FIX["Corriger / Ajuster"] --> GEN
  DECIDE -->|Oui| HUMAN["Revue humaine"] --> COMMIT["Valider + Spec"]
  COMMIT --> MAINT["Maintenance"]
  classDef main fill:#2b6cb0,stroke:#3182ce,color:#e2e8f0
  class SPEC,GEN,TESTS,HUMAN,COMMIT main
```

### Quand privilÃ©gier Spec-Driven vs Vibe Coding

| **CritÃ¨re** | **Spec-Driven** | **Vibe Coding** |
|-------------|----------------|-----------------|
| **Type de projet** | Production, bibliothÃ¨que partagÃ©e | Prototype, POC, exploration |
| **CriticitÃ©** | Code critique (sÃ©curitÃ©, finance) | ExpÃ©rimentation, apprentissage |
| **Ã‰quipe** | Collaboration multi-dev | DÃ©veloppement solo |
| **Documentation** | Requise et maintenue | Optionnelle |
| **Tests** | Complets et automatisÃ©s | Tests exploratoires |
| **Ã‰volutivitÃ©** | Long terme | Court terme |

[ğŸ’« Toolkit to help you get started with Spec-Driven Development](https://github.com/github/spec-kit)

### (Annexe) FenÃªtre de contexte : entrÃ©e / sortie (estimation)

Ce tableau indique, pour chaque modÃ¨le, une estimation de la fenÃªtre totale (tokens), puis une estimation typique de la quantitÃ© maximale utilisable pour l'entrÃ©e (tokens d'entrÃ©e) et pour la sortie gÃ©nÃ©rÃ©e. Rappel : entrÃ©e + sortie â‰¤ fenÃªtre totale.

| ModÃ¨le (exemples)         | FenÃªtre totale (tokens) | Tokens d'entrÃ©e max (approx.) | Tokens sortie max (approx.) | Commentaire |
|---------------------------|------------------------:|------------------------------:|----------------------------:|------------|
| GPT-4.1 (Copilot)         | â‰ˆ128k                  | â‰ˆ96kâ€“120k                     | â‰ˆ8kâ€“32k                     | IntÃ©grÃ© Ã  Copilot, bon pour multi-fichiers et suggestions contextuelles |
| GPT-5 Mini                | â‰ˆ64k                   | â‰ˆ56kâ€“60k                      | â‰ˆ4kâ€“8k                      | Variante allÃ©gÃ©e de GPT-5, Ã©conomique pour usages frÃ©quents |
| Cloud Sonnet 4            | â‰ˆ200k                  | â‰ˆ176kâ€“192k                    | â‰ˆ8kâ€“24k                     | ConÃ§u pour documents volumineux et code long |
| Cloud Sonnet 4.5          | â‰ˆ500k                  | â‰ˆ452kâ€“488k                    | â‰ˆ12kâ€“48k                    | FenÃªtre Ã©tendue pour trÃ¨s grands contextes |
| GPT-5                     | â‰ˆ1M+                   | â‰ˆ800kâ€“950k                    | â‰ˆ50kâ€“200k+                  | Configurations endpoint-dependent ; forte capacitÃ© pour larges projets |
| Grok Code Fast 1         | â‰ˆ64k                   | â‰ˆ56kâ€“60k                      | â‰ˆ4kâ€“8k                      | OptimisÃ© pour le code, faible latence pour suggestions rapides |
| Mistral (dernier)         | â‰ˆ64k (varie)           | â‰ˆ52kâ€“60k                      | â‰ˆ4kâ€“12k                     | Variantes optimisÃ©es pour le code et le coÃ»t |
| Llama 3 (dernier)         | â‰ˆ128kâ€“512k             | â‰ˆ96kâ€“480k                     | â‰ˆ8kâ€“32k                     | Versions et builds variables (open-source/entreprise) |
| Gemini 2.5                | â‰ˆ1M                    | â‰ˆ800kâ€“950k                    | â‰ˆ50kâ€“200k+                  | OrientÃ© trÃ¨s large contexte, utile pour projets multi-fichiers |

Note : estimations (oct. 2025) â€” les valeurs rÃ©elles dÃ©pendent de l'endpoint, des limites imposÃ©es par le fournisseur et des configurations de modÃ¨le. "Tokens d'entrÃ©e max" = fenÃªtre totale âˆ’ tokens rÃ©servÃ©s pour la sortie ; les plages donnÃ©es indiquent des allocations typiques selon usage (conservateur â†’ agressif).

Astuce pratique : si vous avez besoin d'analyser de longs dÃ©pÃ´ts, privilÃ©giez les modÃ¨les Ã  grande fenÃªtre ou prÃ©traitez / rÃ©sumez le code pour n'envoyer que les parties essentielles (signatures, tests, exemples).

---

## ğŸ“‹ RÃ©sumÃ© en 5 minutes

Si vous n'avez retenu qu'une chose de chaque section :

| # | Section | ğŸ’¡ Point clÃ© |
|---|---------|-------------|
| ğŸ¯ | **C'est quoi l'IA** | Copilot est un assistant qui prÃ©dit, il ne pense pas |
| 1 | **En une phrase** | LLM = prÃ©diction de la suite la plus probable |
| 2 | **Tokens** | Plus le prompt est court et prÃ©cis, meilleur est le rÃ©sultat |
| 3 | **Usage concret** | Auto-complÃ©tion, tests, refactoring, documentation |
| 4 | **Contexte** | Donnez des exemples et noms explicites |
| 5 | **Prompts** | 5 ingrÃ©dients : Objectif, Contraintes, EntrÃ©es, Sorties, Exemple |
| 6 | **Limites** | VÃ©rifiez toujours le code gÃ©nÃ©rÃ© ! |
| 7 | **Vibe Coding** | Prototypage rapide avec itÃ©rations courtes |
| 8 | **Spec-Driven** | Pour le code de production, spÃ©cifiez d'abord |

### ğŸš€ PrÃªt pour l'exercice pratique ?

Passez maintenant Ã  la section [GitHub Copilot](COPILOT.md) pour dÃ©couvrir les raccourcis et astuces pratiques, puis lancez-vous dans l'exercice !

---
